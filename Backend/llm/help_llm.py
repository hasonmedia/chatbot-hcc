import json
import re
import time
from typing import List, Dict, Tuple, Optional, Any
from sqlalchemy import text, select, desc
from sqlalchemy.ext.asyncio import AsyncSession
from config.get_embedding import get_embedding_chatgpt, get_embedding_gemini
from models.chat import Message, CustomerInfo
from models.field_config import FieldConfig
from models.llm import LLM
from config.redis_cache import cache_get, cache_set, cache_delete


async def get_llm_keys_cached(llm_id: int, db_session: AsyncSession) -> list:
   
    from models.llm import LLMKey
    from config.redis_cache import async_cache_get, async_cache_set
    
    # Cache key cho danh s√°ch keys
    cache_key = f"llm_keys:llm_id_{llm_id}"
    
    # 1. Th·ª≠ l·∫•y t·ª´ cache tr∆∞·ªõc
    cached_keys = await async_cache_get(cache_key)
    if cached_keys is not None:
        print(f"‚úÖ Cache hit: L·∫•y {len(cached_keys)} keys t·ª´ cache cho LLM id={llm_id}")
        return cached_keys
    
    # 2. N·∫øu kh√¥ng c√≥ trong cache, query t·ª´ database
    result = await db_session.execute(
        select(LLMKey)
        .filter(LLMKey.llm_id == llm_id)
        .order_by(LLMKey.id)  # ƒê·∫£m b·∫£o th·ª© t·ª± c·ªë ƒë·ªãnh
    )
    llm_keys = result.scalars().all()
    
    if not llm_keys:
        raise ValueError(f"Kh√¥ng t√¨m th·∫•y API key n√†o cho LLM id={llm_id}")
    
    # 3. Chuy·ªÉn ƒë·ªïi th√†nh list dict ƒë·ªÉ cache (v√¨ kh√¥ng th·ªÉ cache SQLAlchemy objects)
    keys_data = [
        {"id": key.id, "name": key.name, "key": key.key}
        for key in llm_keys
    ]
    
    # 4. Cache v·ªõi TTL 1 gi·ªù (3600 gi√¢y) - keys √≠t thay ƒë·ªïi
    await async_cache_set(cache_key, keys_data, ttl=3600)
    print(f"üíæ Cache miss: L∆∞u {len(keys_data)} keys v√†o cache cho LLM id={llm_id}")
    
    return keys_data


async def get_round_robin_api_key(
    llm_id: int,
    chat_session_id: int,
    db_session: AsyncSession
) -> tuple[str, str]:
    
    from config.redis_cache import async_cache_get, async_cache_set
    
    try:
        # 1. L·∫•y danh s√°ch t·∫•t c·∫£ c√°c keys c·ªßa LLM n√†y (c√≥ cache)
        llm_keys = await get_llm_keys_cached(llm_id, db_session)
        
        # N·∫øu ch·ªâ c√≥ 1 key, tr·∫£ v·ªÅ lu√¥n
        if len(llm_keys) == 1:
            return llm_keys[0]["key"], llm_keys[0]["name"]
        
        # 2. Ki·ªÉm tra xem chat_session_id n√†y ƒë√£ ƒë∆∞·ª£c g√°n key ch∆∞a
        session_key = f"llm_key_session:llm_{llm_id}:session_{chat_session_id}"
        assigned_index = await async_cache_get(session_key)
        
        if assigned_index is not None:
            # Session ƒë√£ c√≥ key ƒë∆∞·ª£c g√°n, d√πng l·∫°i key ƒë√≥
            selected_index = int(assigned_index)
            selected_key = llm_keys[selected_index]
            print(f"‚úÖ Chat session {chat_session_id} ti·∫øp t·ª•c d√πng key: {selected_key['name']}")
            return selected_key["key"], selected_key["name"]
        
        # 3. Session m·ªõi ch∆∞a c√≥ key, l·∫•y counter to√†n c·ª•c ƒë·ªÉ g√°n key m·ªõi
        counter_key = f"llm_key_global_counter:llm_{llm_id}"
        current_counter = await async_cache_get(counter_key)
        
        if current_counter is None:
            # L·∫ßn ƒë·∫ßu ti√™n, kh·ªüi t·∫°o counter = 0
            current_counter = 0
        else:
            current_counter = int(current_counter)
        
        # 4. T√≠nh index t·ª´ counter (Round-Robin)
        selected_index = current_counter % len(llm_keys)
        
        # 5. TƒÉng counter l√™n 1 cho session ti·∫øp theo
        next_counter = current_counter + 1
        await async_cache_set(counter_key, next_counter, ttl=86400)
        
        # 6. L∆∞u mapping session -> key index (TTL 24 gi·ªù)
        await async_cache_set(session_key, selected_index, ttl=3600)
        
        # 7. Tr·∫£ v·ªÅ API key t∆∞∆°ng ·ª©ng
        selected_key = llm_keys[selected_index]
        print(f"üîÑ Chat session {chat_session_id} ƒë∆∞·ª£c g√°n key m·ªõi: {selected_key['name']}")
        
        return selected_key["key"], selected_key["name"]
        
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y Round-Robin API key: {e}")
        raise


async def get_llm_model_info_cached(db_session: AsyncSession) -> dict:
    
    from config.redis_cache import async_cache_get, async_cache_set
    
    # Cache key cho th√¥ng tin model
    cache_key = "llm_model_info:id_1"
    
    # 1. Th·ª≠ l·∫•y t·ª´ cache tr∆∞·ªõc
    cached_model = await async_cache_get(cache_key)
    if cached_model is not None:
        print(f"‚úÖ Cache hit: L·∫•y th√¥ng tin model t·ª´ cache")
        return cached_model
    
    # 2. N·∫øu kh√¥ng c√≥ trong cache, query t·ª´ database
    result = await db_session.execute(select(LLM).where(LLM.id == 1))
    model = result.scalars().first()

    if not model:
        raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y model c√≥ id = 1 trong b·∫£ng LLM")
    
    # 3. T·∫°o model data
    model_data = {
        "id": model.id,
        "name": model.name,
        "key": model.key
    }
    
    # 4. Cache v·ªõi TTL 1 gi·ªù (3600 gi√¢y) - model config √≠t thay ƒë·ªïi
    await async_cache_set(cache_key, model_data, ttl=3600)
    print(f"üíæ Cache miss: L∆∞u th√¥ng tin model v√†o cache")
    
    return model_data


async def get_current_model(db_session: AsyncSession, chat_session_id: int = None) -> dict:
    """
    L·∫•y th√¥ng tin model hi·ªán t·∫°i t·ª´ database v·ªõi Redis cache
    
    Args:
        db_session: AsyncSession - Database session
        chat_session_id: int (optional) - ID c·ªßa chat session. 
                         N·∫øu c√≥, s·∫Ω s·ª≠ d·ª•ng Round-Robin ƒë·ªÉ ch·ªçn API key
    
    Returns:
        dict - Dictionary ch·ª©a th√¥ng tin model:
            - name: str - T√™n model (gpt, gemini, etc.)
            - key: str - API key c·ªßa model (t·ª´ llm_key n·∫øu c√≥ chat_session_id, 
                        ho·∫∑c t·ª´ llm.key n·∫øu kh√¥ng)
            - key_name: str (optional) - T√™n c·ªßa key ƒë∆∞·ª£c ch·ªçn (ch·ªâ c√≥ khi d√πng Round-Robin)
    
    Raises:
        ValueError - N·∫øu kh√¥ng t√¨m th·∫•y model c√≥ id = 1
    """
    try:
        # L·∫•y th√¥ng tin LLM model t·ª´ cache (gi·∫£m thi·ªÉu query DB)
        model_info = await get_llm_model_info_cached(db_session)

        # N·∫øu c√≥ chat_session_id, s·ª≠ d·ª•ng Round-Robin ƒë·ªÉ ch·ªçn key t·ª´ llm_key
        if chat_session_id is not None:
            try:
                api_key, key_name = await get_round_robin_api_key(model_info["id"], chat_session_id, db_session)
                model_data = {
                    "name": model_info["name"], 
                    "key": api_key,
                    "key_name": key_name
                }
                return model_data
            except ValueError as e:
                # N·∫øu kh√¥ng c√≥ key trong llm_key, fallback v·ªÅ key m·∫∑c ƒë·ªãnh t·ª´ b·∫£ng llm
                print(f"‚ö†Ô∏è Fallback to default key: {e}")
                model_data = {
                    "name": model_info["name"], 
                    "key": model_info["key"],
                    "key_name": "default"
                }
                return model_data
        else:
            # Kh√¥ng c√≥ chat_session_id, tr·∫£ v·ªÅ key m·∫∑c ƒë·ªãnh t·ª´ b·∫£ng llm
            model_data = {
                "name": model_info["name"], 
                "key": model_info["key"]
            }
            return model_data
            
    except Exception as e:
        print(f"‚ùå Error getting current model: {e}")
        raise

async def get_latest_messages(
    db_session: AsyncSession, 
    chat_session_id: int, 
    limit: int
) -> str:
    """
    L·∫•y l·ªãch s·ª≠ tin nh·∫Øn g·∫ßn ƒë√¢y t·ª´ database
    
    Args:
        db_session: AsyncSession - Database session
        chat_session_id: int - ID c·ªßa chat session
        limit: int - S·ªë l∆∞·ª£ng tin nh·∫Øn t·ªëi ƒëa c·∫ßn l·∫•y
    
    Returns:
        str - L·ªãch s·ª≠ h·ªôi tho·∫°i d∆∞·ªõi d·∫°ng text, m·ªói d√≤ng format: "sender_type: content"
    """
    result = await db_session.execute(
        select(Message)
        .filter(Message.chat_session_id == chat_session_id)
        .order_by(desc(Message.created_at))
        .limit(limit)
    )
    messages = result.scalars().all()
    
    results = [
        {
            "id": m.id,
            "content": m.content,
            "sender_type": m.sender_type,
            "created_at": m.created_at.isoformat() if m.created_at else None
        }
        for m in reversed(messages) 
    ]

    conversation = []
    for msg in results:
        line = f"{msg['sender_type']}: {msg['content']}"
        conversation.append(line)
    
    conversation_text = "\n".join(conversation)
    
    return conversation_text


async def build_search_key(
    model,
    db_session: AsyncSession,
    chat_session_id: int, 
    question: str, 
    customer_info: Optional[dict] = None
) -> str:
    """
    T·∫°o t·ª´ kh√≥a t√¨m ki·∫øm t·ªëi ∆∞u t·ª´ c√¢u h·ªèi c·ªßa kh√°ch h√†ng
    S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch context v√† t·∫°o search key ph√π h·ª£p
    
    Args:
        model: LLM model (Gemini ho·∫∑c GPT) - model ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
        db_session: AsyncSession - Database session
        chat_session_id: int - ID c·ªßa chat session
        question: str - C√¢u h·ªèi c·ªßa kh√°ch h√†ng
        customer_info: dict - Th√¥ng tin kh√°ch h√†ng (optional)
    
    Returns:
        str - T·ª´ kh√≥a t√¨m ki·∫øm ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u
    """
    history = await get_latest_messages(db_session, chat_session_id, limit=5)
    
    # Chu·∫©n b·ªã th√¥ng tin kh√°ch h√†ng cho context
    customer_context = ""
    if customer_info:
        customer_context = f"\nTh√¥ng tin kh√°ch h√†ng: {customer_info}"
    
    # Import prompt t·ª´ file prompt_search_key.py
    from llm.prompt_search_key import get_search_key_prompt
    
    # T·∫°o prompt
    prompt = get_search_key_prompt(history, customer_context, question)
    
    # G·ªçi model t√πy theo lo·∫°i (Gemini ho·∫∑c GPT)
    if hasattr(model, 'generate_content'):
        # C·∫£ GPT v√† Gemini ƒë·ªÅu c√≥ generate_content, nh∆∞ng GPT l√† async
        if hasattr(model, 'client'):
            # GPTModel - async function
            response_text = await model.generate_content(prompt)
            return response_text.strip()
        else:
            # GeminiModel - sync function
            response = model.generate_content(prompt)
            return response.text.strip()
    else:
        # Fallback cho c√°c model kh√°c
        response = await model.chat.completions.create(
            model=model.model_name if hasattr(model, 'model_name') else "gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content.strip()


async def search_similar_documents(
    db_session: AsyncSession,
    query: str, 
    top_k: int,
    api_key: str = None,
    model_name: str = None
) -> List[Dict]:
    
    try:
        if "gemini" in model_name:
            query_embedding = await get_embedding_gemini(query, api_key=api_key)
        else: 
            query_embedding = await get_embedding_chatgpt(query, api_key=api_key)
        
        if query_embedding is None:
            print("‚ö†Ô∏è Failed to create embedding for query")
            return []

        query_embedding = query_embedding.tolist()
        query_embedding = "[" + ",".join([str(x) for x in query_embedding]) + "]"

        sql = text("""
            SELECT id, chunk_text, search_vector <-> (:query_embedding)::vector AS similarity
            FROM document_chunks
            ORDER BY search_vector <-> (:query_embedding)::vector
            LIMIT :top_k
        """)

        result = await db_session.execute(
            sql, {"query_embedding": query_embedding, "top_k": top_k}
        )
        rows = result.fetchall()

        results = []
        for row in rows:
            results.append({
                "content": row.chunk_text
            })

        return results

    except Exception as e:
        raise Exception(f"L·ªói khi t√¨m ki·∫øm: {str(e)}")


async def get_field_configs(db_session: AsyncSession) -> Tuple[Dict[str, str], Dict[str, str]]:
   
    cache_key = "field_configs:required_optional"
    
    # Th·ª≠ l·∫•y t·ª´ cache tr∆∞·ªõc
    cached_result = cache_get(cache_key)
    if cached_result is not None:
        return cached_result.get('required_fields', {}), cached_result.get('optional_fields', {})
    
    try:
        result = await db_session.execute(
            select(FieldConfig).order_by(FieldConfig.excel_column_letter)
        )
        field_configs = result.scalars().all()
        
        required_fields = {}
        optional_fields = {}
        
        for config in field_configs:
            field_name = config.excel_column_name
            if config.is_required:
                required_fields[field_name] = field_name
            else:
                optional_fields[field_name] = field_name
        
        # Cache k·∫øt qu·∫£ v·ªõi TTL 24 gi·ªù (86400 gi√¢y)
        cache_data = {
            'required_fields': required_fields,
            'optional_fields': optional_fields
        }
        cache_set(cache_key, cache_data, ttl=86400)
                
        return required_fields, optional_fields
    except Exception as e:
        print(f"L·ªói khi l·∫•y field configs: {str(e)}")
        # Tr·∫£ v·ªÅ dict r·ªóng n·∫øu c√≥ l·ªói
        return {}, {}


async def get_customer_infor(db_session: AsyncSession, chat_session_id: int) -> dict:
    """
    L·∫•y th√¥ng tin kh√°ch h√†ng t·ª´ database
    
    Args:
        db_session: AsyncSession - Database session
        chat_session_id: int - ID c·ªßa chat session
    
    Returns:
        dict - Th√¥ng tin kh√°ch h√†ng d∆∞·ªõi d·∫°ng dictionary
               Tr·∫£ v·ªÅ {} n·∫øu kh√¥ng c√≥ th√¥ng tin ho·∫∑c c√≥ l·ªói
    """
    try:
        # L·∫•y th√¥ng tin kh√°ch h√†ng t·ª´ b·∫£ng customer_info
        result = await db_session.execute(
            select(CustomerInfo).filter(CustomerInfo.chat_session_id == chat_session_id)
        )
        customer_info = result.scalar_one_or_none()
        
        if customer_info and customer_info.customer_data:
            # N·∫øu customer_data l√† string JSON, parse n√≥
            if isinstance(customer_info.customer_data, str):
                return json.loads(customer_info.customer_data)
            # N·∫øu ƒë√£ l√† dict th√¨ return tr·ª±c ti·∫øp
            return customer_info.customer_data
        return {}
    except Exception as e:
        print(f"L·ªói khi l·∫•y th√¥ng tin kh√°ch h√†ng: {str(e)}")
        return {}


async def extract_customer_info_realtime(
    model,
    db_session: AsyncSession,
    chat_session_id: int, 
    limit_messages: int
) -> Optional[str]:
    """
    Tr√≠ch xu·∫•t th√¥ng tin kh√°ch h√†ng real-time t·ª´ l·ªãch s·ª≠ h·ªôi tho·∫°i
    S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch v√† tr√≠ch xu·∫•t th√¥ng tin theo c·∫•u h√¨nh fields
    
    Args:
        model: LLM model (Gemini ho·∫∑c GPT) - model ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
        db_session: AsyncSession - Database session
        chat_session_id: int - ID c·ªßa chat session
        limit_messages: int - S·ªë l∆∞·ª£ng tin nh·∫Øn g·∫ßn ƒë√¢y c·∫ßn ph√¢n t√≠ch
    
    Returns:
        str - JSON string ch·ª©a th√¥ng tin kh√°ch h√†ng ƒë√£ tr√≠ch xu·∫•t
              Tr·∫£ v·ªÅ None n·∫øu c√≥ l·ªói
    """
    try:
        history = await get_latest_messages(db_session, chat_session_id, limit_messages)
        
        # L·∫•y c·∫•u h√¨nh fields ƒë·ªông
        required_fields, optional_fields = await get_field_configs(db_session)
        all_fields = {**required_fields, **optional_fields}
        
        # N·∫øu kh√¥ng c√≥ field configs, tr·∫£ v·ªÅ JSON r·ªóng
        if not all_fields:
            return json.dumps({})
        
        # N·∫øu kh√¥ng c√≥ l·ªãch s·ª≠ h·ªôi tho·∫°i, tr·∫£ v·ªÅ JSON r·ªóng v·ªõi c√°c fields t·ª´ config
        if not history or history.strip() == "":
            empty_json = {field_name: None for field_name in all_fields.values()}
            return json.dumps(empty_json)
        
        # T·∫°o danh s√°ch fields cho prompt - ch·ªâ c√°c fields t·ª´ field_config
        fields_description = "\n".join([
            f"- {field_name}: tr√≠ch xu·∫•t {field_name.lower()} t·ª´ h·ªôi tho·∫°i"
            for field_name in all_fields.values()
        ])
        
        # T·∫°o v√≠ d·ª• JSON template - ch·ªâ c√°c fields t·ª´ field_config
        example_json = {field_name: f"<{field_name}>" for field_name in all_fields.values()}
        example_json_str = json.dumps(example_json, ensure_ascii=False, indent=4)
        
        prompt = f"""
            B·∫°n l√† m·ªôt c√¥ng c·ª• ph√¢n t√≠ch h·ªôi tho·∫°i ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin kh√°ch h√†ng.

            D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
            {history}

            H√£y tr√≠ch xu·∫•t TO√ÄN B·ªò th√¥ng tin kh√°ch h√†ng c√≥ trong h·ªôi tho·∫°i v√† tr·∫£ v·ªÅ JSON v·ªõi C√ÅC TR∆Ø·ªúNG SAU (ch·ªâ c√°c tr∆∞·ªùng n√†y):
            {fields_description}

            QUY T·∫ÆC QUAN TR·ªåNG:
            - CH·ªà tr√≠ch xu·∫•t c√°c tr∆∞·ªùng ƒë∆∞·ª£c li·ªát k√™ ·ªü tr√™n
            - KH√îNG th√™m b·∫•t k·ª≥ tr∆∞·ªùng n√†o kh√°c (nh∆∞ registration, status, etc.)
            - N·∫øu kh√¥ng c√≥ th√¥ng tin cho tr∆∞·ªùng n√†o th√¨ ƒë·ªÉ null
            - CH·ªà tr·∫£ v·ªÅ JSON thu·∫ßn t√∫y, kh√¥ng c√≥ text kh√°c
            - Kh√¥ng s·ª≠ d·ª•ng markdown formatting
            - JSON ph·∫£i h·ª£p l·ªá ƒë·ªÉ d√πng v·ªõi json.loads()

            V√≠ d·ª• format tr·∫£ v·ªÅ (ch·ªâ ch·ª©a c√°c tr∆∞·ªùng t·ª´ c·∫•u h√¨nh):
            {example_json_str}
            """
        
        # G·ªçi model t√πy theo lo·∫°i (Gemini ho·∫∑c GPT)
        if hasattr(model, 'generate_content'):
            # C·∫£ GPT v√† Gemini ƒë·ªÅu c√≥ generate_content, nh∆∞ng GPT l√† async
            if hasattr(model, 'client'):
                # GPTModel - async function
                response_text = await model.generate_content(prompt)
                cleaned = re.sub(r"```json|```", "", response_text).strip()
            else:
                # GeminiModel - sync function
                response = model.generate_content(prompt)
                cleaned = re.sub(r"```json|```", "", response.text).strip()
        else:
            # Fallback cho c√°c model kh√°c
            response = await model.chat.completions.create(
                model=model.model_name if hasattr(model, 'model_name') else "gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            cleaned = re.sub(r"```json|```", "", response.choices[0].message.content).strip()
        
        return cleaned
        
    except Exception as e:
        print(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin kh√°ch h√†ng: {str(e)}")
        return None


def clear_field_configs_cache() -> bool:
    """
    X√≥a cache field configs khi c√≥ thay ƒë·ªïi c·∫•u h√¨nh
    
    Returns:
        bool - True n·∫øu x√≥a cache th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    cache_key = "field_configs:required_optional"
    success = cache_delete(cache_key)
    return success


async def clear_llm_keys_cache(llm_id: int = None) -> bool:
    """
    X√≥a cache danh s√°ch API keys khi c√≥ thay ƒë·ªïi (th√™m, s·ª≠a, x√≥a key)
    
    Args:
        llm_id: ID c·ªßa LLM model. N·∫øu None, x√≥a cache cho t·∫•t c·∫£ LLMs
    
    Returns:
        bool - True n·∫øu x√≥a cache th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    from config.redis_cache import async_cache_delete
    
    try:
        if llm_id is not None:
            # X√≥a cache cho m·ªôt LLM c·ª• th·ªÉ
            cache_key = f"llm_keys:llm_id_{llm_id}"
            success = await async_cache_delete(cache_key)
            return success
        else:
            # X√≥a cache cho t·∫•t c·∫£ (c√≥ th·ªÉ d√πng Redis pattern matching n·∫øu c·∫ßn)
            # Hi·ªán t·∫°i ch·ªâ x√≥a cho LLM id=1 (model ch√≠nh)
            cache_key = "llm_keys:llm_id_1"
            success = await async_cache_delete(cache_key)
            print(f"üóëÔ∏è ƒê√£ x√≥a cache keys cho t·∫•t c·∫£ LLMs")
            return success
    except Exception as e:
        print(f"‚ùå L·ªói khi x√≥a cache keys: {e}")
        return False


async def clear_llm_model_cache() -> bool:
    """
    X√≥a cache th√¥ng tin model khi c√≥ thay ƒë·ªïi (c·∫≠p nh·∫≠t name, key m·∫∑c ƒë·ªãnh, etc.)
    
    Returns:
        bool - True n·∫øu x√≥a cache th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    from config.redis_cache import async_cache_delete
    
    try:
        cache_key = "llm_model_info:id_1"
        success = await async_cache_delete(cache_key)
        return success
    except Exception as e:
        print(f"‚ùå L·ªói khi x√≥a cache model: {e}")
        return False


async def generate_response_prompt(
    model,
    db_session: AsyncSession,
    query: str,
    chat_session_id: int,
    api_key_for_embedding: str = None,
    model_name: str = None
) -> str:
   
    try:
        # L·∫•y l·ªãch s·ª≠ v√† th√¥ng tin kh√°ch h√†ng
        history = await get_latest_messages(db_session, chat_session_id, limit=10)
        customer_info = await get_customer_infor(db_session, chat_session_id)
        
        if not query or query.strip() == "":
            return "N·ªôi dung c√¢u h·ªèi tr·ªëng, vui l√≤ng nh·∫≠p l·∫°i."
        
        # T·∫°o search key
        search_key = await build_search_key(
            model=model,
            db_session=db_session,
            chat_session_id=chat_session_id,
            question=query,
            customer_info=customer_info
        )
        print(f"üîç Search key: {search_key}")
        
        # T√¨m ki·∫øm t√†i li·ªáu li√™n quan (truy·ªÅn model_name ƒë·ªÉ tr√°nh query DB th√™m l·∫ßn n·ªØa)
        knowledge = await search_similar_documents(
            db_session, 
            search_key, 
            top_k=10,
            api_key=api_key_for_embedding,
            model_name=model_name  # Truy·ªÅn model_name ƒë·ªÉ tr√°nh g·ªçi get_current_model()
        )
        
        
        # L·∫•y c·∫•u h√¨nh fields
        required_fields, optional_fields = await get_field_configs(db_session)
        
        # T·∫°o danh s√°ch th√¥ng tin c·∫ßn thu th·∫≠p
        required_info_list = "\n".join([f"- {field_name} (b·∫Øt bu·ªôc)" for field_name in required_fields.values()])
        optional_info_list = "\n".join([f"- {field_name} (t√πy ch·ªçn)" for field_name in optional_fields.values()])
        
        # Import prompt_builder t·ª´ prompt.py
        from llm.prompt import prompt_builder
        
        # T·∫°o prompt
        prompt = await prompt_builder(
            knowledge=knowledge,
            customer_info=customer_info,
            required_info_list=required_info_list,
            optional_info_list=optional_info_list,
            history=history,
            query=query
        )
        
        return prompt
        
    except Exception as e:
        print(f"‚ùå Error generating response: {e}")
        return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n: {str(e)}"




