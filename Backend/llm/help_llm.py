import json
import re
import time
from typing import List, Dict, Tuple, Optional, Any
from sqlalchemy import text, select, desc
from sqlalchemy.ext.asyncio import AsyncSession
from config.get_embedding import get_embedding_chatgpt, get_embedding_gemini
from models.chat import Message, CustomerInfo
from models.field_config import FieldConfig
from models.llm import LLM
from config.redis_cache import cache_get, cache_set, cache_delete


async def get_llm_keys_cached(llm_id: int, db_session: AsyncSession, key_type: str = None) -> list:
    """
    L·∫•y danh s√°ch API keys t·ª´ cache ho·∫∑c database
    
    Args:
        llm_id: ID c·ªßa LLM model
        db_session: AsyncSession - Database session
        key_type: Lo·∫°i key c·∫ßn l·∫•y ("bot" ho·∫∑c "embedding"). N·∫øu None, l·∫•y t·∫•t c·∫£
    
    Returns:
        list - Danh s√°ch c√°c keys
    """
    from models.llm import LLMKey
    from config.redis_cache import async_cache_get, async_cache_set
    
    # Cache key cho danh s√°ch keys (bao g·ªìm c·∫£ type trong cache key)
    cache_key = f"llm_keys:llm_id_{llm_id}:type_{key_type or 'all'}"
    
    # 1. Th·ª≠ l·∫•y t·ª´ cache tr∆∞·ªõc
    cached_keys = await async_cache_get(cache_key)
    if cached_keys is not None:
        print(f"‚úÖ Cache hit: L·∫•y {len(cached_keys)} {key_type or 'all'} keys t·ª´ cache cho LLM id={llm_id}")
        return cached_keys
    
    # 2. N·∫øu kh√¥ng c√≥ trong cache, query t·ª´ database
    query = select(LLMKey).filter(LLMKey.llm_id == llm_id)
    
    # Th√™m filter theo type n·∫øu c√≥
    if key_type:
        query = query.filter(LLMKey.type == key_type)
    
    query = query.order_by(LLMKey.id)  # ƒê·∫£m b·∫£o th·ª© t·ª± c·ªë ƒë·ªãnh
    
    result = await db_session.execute(query)
    llm_keys = result.scalars().all()
    
    if not llm_keys:
        raise ValueError(f"Kh√¥ng t√¨m th·∫•y API key {key_type or ''} n√†o cho LLM id={llm_id}")
    
    # 3. Chuy·ªÉn ƒë·ªïi th√†nh list dict ƒë·ªÉ cache (v√¨ kh√¥ng th·ªÉ cache SQLAlchemy objects)
    keys_data = [
        {"id": key.id, "name": key.name, "key": key.key, "type": key.type}
        for key in llm_keys
    ]
    
    # 4. Cache v·ªõi TTL 1 gi·ªù (3600 gi√¢y) - keys √≠t thay ƒë·ªïi
    await async_cache_set(cache_key, keys_data, ttl=3600)
    print(f"üíæ Cache miss: L∆∞u {len(keys_data)} {key_type or 'all'} keys v√†o cache cho LLM id={llm_id}")
    
    return keys_data


async def get_round_robin_api_key(
    llm_id: int,
    chat_session_id: int,
    db_session: AsyncSession,
    key_type: str = "bot"
) -> tuple[str, str]:
    """
    L·∫•y API key theo thu·∫≠t to√°n Round-Robin cho m·ªói chat session
    
    Args:
        llm_id: ID c·ªßa LLM model
        chat_session_id: ID c·ªßa chat session
        db_session: AsyncSession - Database session
        key_type: Lo·∫°i key c·∫ßn l·∫•y ("bot" ho·∫∑c "embedding"), m·∫∑c ƒë·ªãnh "bot"
    
    Returns:
        tuple[str, str] - (api_key, key_name)
    """
    from config.redis_cache import async_cache_get, async_cache_set
    
    try:
        # 1. L·∫•y danh s√°ch t·∫•t c·∫£ c√°c keys c·ªßa LLM n√†y theo type (c√≥ cache)
        llm_keys = await get_llm_keys_cached(llm_id, db_session, key_type=key_type)
        
        # N·∫øu ch·ªâ c√≥ 1 key, tr·∫£ v·ªÅ lu√¥n
        if len(llm_keys) == 1:
            return llm_keys[0]["key"], llm_keys[0]["name"]
        
        # 2. Ki·ªÉm tra xem chat_session_id n√†y ƒë√£ ƒë∆∞·ª£c g√°n key ch∆∞a (theo type)
        session_key = f"llm_key_session:llm_{llm_id}:session_{chat_session_id}:type_{key_type}"
        assigned_index = await async_cache_get(session_key)
        
        if assigned_index is not None:
            # Session ƒë√£ c√≥ key ƒë∆∞·ª£c g√°n, d√πng l·∫°i key ƒë√≥
            selected_index = int(assigned_index)
            selected_key = llm_keys[selected_index]
            print(f"‚úÖ Chat session {chat_session_id} ti·∫øp t·ª•c d√πng {key_type} key: {selected_key['name']}")
            return selected_key["key"], selected_key["name"]
        
        # 3. Session m·ªõi ch∆∞a c√≥ key, l·∫•y counter to√†n c·ª•c ƒë·ªÉ g√°n key m·ªõi (theo type)
        counter_key = f"llm_key_global_counter:llm_{llm_id}:type_{key_type}"
        current_counter = await async_cache_get(counter_key)
        
        if current_counter is None:
            # L·∫ßn ƒë·∫ßu ti√™n, kh·ªüi t·∫°o counter = 0
            current_counter = 0
        else:
            current_counter = int(current_counter)
        
        # 4. T√≠nh index t·ª´ counter (Round-Robin)
        selected_index = current_counter % len(llm_keys)
        
        # 5. TƒÉng counter l√™n 1 cho session ti·∫øp theo
        next_counter = current_counter + 1
        await async_cache_set(counter_key, next_counter, ttl=86400)
        
        # 6. L∆∞u mapping session -> key index (TTL 24 gi·ªù)
        await async_cache_set(session_key, selected_index, ttl=3600)
        
        # 7. Tr·∫£ v·ªÅ API key t∆∞∆°ng ·ª©ng
        selected_key = llm_keys[selected_index]
        print(f"üîÑ Chat session {chat_session_id} ƒë∆∞·ª£c g√°n {key_type} key m·ªõi: {selected_key['name']}")
        
        return selected_key["key"], selected_key["name"]
        
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y Round-Robin API key ({key_type}): {e}")
        raise


async def get_llm_model_info_cached(db_session: AsyncSession) -> dict:
    
    from config.redis_cache import async_cache_get, async_cache_set
    
    # Cache key cho th√¥ng tin model
    cache_key = "llm_model_info:id_1"
    
    # 1. Th·ª≠ l·∫•y t·ª´ cache tr∆∞·ªõc
    cached_model = await async_cache_get(cache_key)
    if cached_model is not None:
        print(f"‚úÖ Cache hit: L·∫•y th√¥ng tin model t·ª´ cache")
        return cached_model
    
    # 2. N·∫øu kh√¥ng c√≥ trong cache, query t·ª´ database
    result = await db_session.execute(select(LLM).where(LLM.id == 1))
    model = result.scalars().first()

    if not model:
        raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y model c√≥ id = 1 trong b·∫£ng LLM")
    
    # 3. T·∫°o model data
    model_data = {
        "id": model.id,
        "name": model.name,
        "key": model.key
    }
    
    # 4. Cache v·ªõi TTL 1 gi·ªù (3600 gi√¢y) - model config √≠t thay ƒë·ªïi
    await async_cache_set(cache_key, model_data, ttl=3600)
    print(f"üíæ Cache miss: L∆∞u th√¥ng tin model v√†o cache")
    
    return model_data


async def get_current_model(db_session: AsyncSession, chat_session_id: int = None, key_type: str = "bot") -> dict:
    """
    L·∫•y th√¥ng tin model hi·ªán t·∫°i v√† API key ph√π h·ª£p
    
    Args:
        db_session: AsyncSession - Database session
        chat_session_id: int - ID c·ªßa chat session (optional)
        key_type: str - Lo·∫°i key c·∫ßn l·∫•y ("bot" ho·∫∑c "embedding"), m·∫∑c ƒë·ªãnh "bot"
    
    Returns:
        dict - Th√¥ng tin model bao g·ªìm name, key, key_name
    """
    try:
        # L·∫•y th√¥ng tin LLM model t·ª´ cache (gi·∫£m thi·ªÉu query DB)
        model_info = await get_llm_model_info_cached(db_session)

        # N·∫øu c√≥ chat_session_id, s·ª≠ d·ª•ng Round-Robin ƒë·ªÉ ch·ªçn key t·ª´ llm_key theo type
        if chat_session_id is not None:
            try:
                api_key, key_name = await get_round_robin_api_key(
                    model_info["id"], 
                    chat_session_id, 
                    db_session,
                    key_type=key_type
                )
                model_data = {
                    "name": model_info["name"], 
                    "key": api_key,
                    "key_name": key_name,
                    "key_type": key_type
                }
                return model_data
            except ValueError as e:
                # N·∫øu kh√¥ng c√≥ key trong llm_key, fallback v·ªÅ key m·∫∑c ƒë·ªãnh t·ª´ b·∫£ng llm
                print(f"‚ö†Ô∏è Fallback to default key for {key_type}: {e}")
                model_data = {
                    "name": model_info["name"], 
                    "key": model_info["key"],
                    "key_name": "default",
                    "key_type": key_type
                }
                return model_data
        else:
            # Kh√¥ng c√≥ chat_session_id, tr·∫£ v·ªÅ key m·∫∑c ƒë·ªãnh t·ª´ b·∫£ng llm
            model_data = {
                "name": model_info["name"], 
                "key": model_info["key"],
                "key_type": key_type
            }
            return model_data
            
    except Exception as e:
        print(f"‚ùå Error getting current model: {e}")
        raise

async def get_latest_messages(
    db_session: AsyncSession, 
    chat_session_id: int, 
    limit: int
) -> str:
    """
    L·∫•y l·ªãch s·ª≠ tin nh·∫Øn g·∫ßn ƒë√¢y t·ª´ database
    
    Args:
        db_session: AsyncSession - Database session
        chat_session_id: int - ID c·ªßa chat session
        limit: int - S·ªë l∆∞·ª£ng tin nh·∫Øn t·ªëi ƒëa c·∫ßn l·∫•y
    
    Returns:
        str - L·ªãch s·ª≠ h·ªôi tho·∫°i d∆∞·ªõi d·∫°ng text, m·ªói d√≤ng format: "sender_type: content"
    """
    result = await db_session.execute(
        select(Message)
        .filter(Message.chat_session_id == chat_session_id)
        .order_by(desc(Message.created_at))
        .limit(limit)
    )
    messages = result.scalars().all()
    
    results = [
        {
            "id": m.id,
            "content": m.content,
            "sender_type": m.sender_type,
            "created_at": m.created_at.isoformat() if m.created_at else None
        }
        for m in reversed(messages) 
    ]

    conversation = []
    for msg in results:
        line = f"{msg['sender_type']}: {msg['content']}"
        conversation.append(line)
    
    conversation_text = "\n".join(conversation)
    
    return conversation_text


async def build_search_key(
    model,
    db_session: AsyncSession,
    chat_session_id: int, 
    question: str, 
    customer_info: Optional[dict] = None
) -> str:
    """
    T·∫°o t·ª´ kh√≥a t√¨m ki·∫øm t·ªëi ∆∞u t·ª´ c√¢u h·ªèi c·ªßa kh√°ch h√†ng
    S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch context v√† t·∫°o search key ph√π h·ª£p
    
    Args:
        model: LLM model (Gemini ho·∫∑c GPT) - model ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
        db_session: AsyncSession - Database session
        chat_session_id: int - ID c·ªßa chat session
        question: str - C√¢u h·ªèi c·ªßa kh√°ch h√†ng
        customer_info: dict - Th√¥ng tin kh√°ch h√†ng (optional)
    
    Returns:
        str - T·ª´ kh√≥a t√¨m ki·∫øm ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u
    """
    history = await get_latest_messages(db_session, chat_session_id, limit=5)
    
    # Chu·∫©n b·ªã th√¥ng tin kh√°ch h√†ng cho context
    customer_context = ""
    if customer_info:
        customer_context = f"\nTh√¥ng tin kh√°ch h√†ng: {customer_info}"
    
    # Import prompt t·ª´ file prompt_search_key.py
    from llm.prompt_search_key import get_search_key_prompt
    
    # T·∫°o prompt
    prompt = get_search_key_prompt(history, customer_context, question)
    
    # G·ªçi model t√πy theo lo·∫°i (Gemini ho·∫∑c GPT)
    if hasattr(model, 'generate_content'):
        # C·∫£ GPT v√† Gemini ƒë·ªÅu c√≥ generate_content, nh∆∞ng GPT l√† async
        if hasattr(model, 'client'):
            # GPTModel - async function
            response_text = await model.generate_content(prompt)
            return response_text.strip()
        else:
            # GeminiModel - sync function
            response = model.generate_content(prompt)
            return response.text.strip()
    else:
        # Fallback cho c√°c model kh√°c
        response = await model.chat.completions.create(
            model=model.model_name if hasattr(model, 'model_name') else "gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content.strip()


async def search_similar_documents(
    db_session: AsyncSession,
    query: str, 
    top_k: int,
    api_key: str = None,
    model_name: str = None
) -> List[Dict]:
    
    try:
        if "gemini" in model_name:
            query_embedding = await get_embedding_gemini(query, api_key=api_key)
        else: 
            query_embedding = await get_embedding_chatgpt(query, api_key=api_key)
        
        if query_embedding is None:
            print("‚ö†Ô∏è Failed to create embedding for query")
            return []

        query_embedding = query_embedding.tolist()
        query_embedding = "[" + ",".join([str(x) for x in query_embedding]) + "]"

        sql = text("""
            SELECT id, chunk_text, search_vector <-> (:query_embedding)::vector AS similarity
            FROM document_chunks
            ORDER BY search_vector <-> (:query_embedding)::vector
            LIMIT :top_k
        """)

        result = await db_session.execute(
            sql, {"query_embedding": query_embedding, "top_k": top_k}
        )
        rows = result.fetchall()

        results = []
        for row in rows:
            results.append({
                "content": row.chunk_text
            })

        return results

    except Exception as e:
        raise Exception(f"L·ªói khi t√¨m ki·∫øm: {str(e)}")


async def get_field_configs(db_session: AsyncSession) -> Tuple[Dict[str, str], Dict[str, str]]:
   
    cache_key = "field_configs:required_optional"
    
    # Th·ª≠ l·∫•y t·ª´ cache tr∆∞·ªõc
    cached_result = cache_get(cache_key)
    if cached_result is not None:
        return cached_result.get('required_fields', {}), cached_result.get('optional_fields', {})
    
    try:
        result = await db_session.execute(
            select(FieldConfig).order_by(FieldConfig.excel_column_letter)
        )
        field_configs = result.scalars().all()
        
        required_fields = {}
        optional_fields = {}
        
        for config in field_configs:
            field_name = config.excel_column_name
            if config.is_required:
                required_fields[field_name] = field_name
            else:
                optional_fields[field_name] = field_name
        
        # Cache k·∫øt qu·∫£ v·ªõi TTL 24 gi·ªù (86400 gi√¢y)
        cache_data = {
            'required_fields': required_fields,
            'optional_fields': optional_fields
        }
        cache_set(cache_key, cache_data, ttl=86400)
                
        return required_fields, optional_fields
    except Exception as e:
        print(f"L·ªói khi l·∫•y field configs: {str(e)}")
        # Tr·∫£ v·ªÅ dict r·ªóng n·∫øu c√≥ l·ªói
        return {}, {}


async def get_customer_infor(db_session: AsyncSession, chat_session_id: int) -> dict:
    """
    L·∫•y th√¥ng tin kh√°ch h√†ng t·ª´ database
    
    Args:
        db_session: AsyncSession - Database session
        chat_session_id: int - ID c·ªßa chat session
    
    Returns:
        dict - Th√¥ng tin kh√°ch h√†ng d∆∞·ªõi d·∫°ng dictionary
               Tr·∫£ v·ªÅ {} n·∫øu kh√¥ng c√≥ th√¥ng tin ho·∫∑c c√≥ l·ªói
    """
    try:
        # L·∫•y th√¥ng tin kh√°ch h√†ng t·ª´ b·∫£ng customer_info
        result = await db_session.execute(
            select(CustomerInfo).filter(CustomerInfo.chat_session_id == chat_session_id)
        )
        customer_info = result.scalar_one_or_none()
        
        if customer_info and customer_info.customer_data:
            # N·∫øu customer_data l√† string JSON, parse n√≥
            if isinstance(customer_info.customer_data, str):
                return json.loads(customer_info.customer_data)
            # N·∫øu ƒë√£ l√† dict th√¨ return tr·ª±c ti·∫øp
            return customer_info.customer_data
        return {}
    except Exception as e:
        print(f"L·ªói khi l·∫•y th√¥ng tin kh√°ch h√†ng: {str(e)}")
        return {}





def clear_field_configs_cache() -> bool:
    """
    X√≥a cache field configs khi c√≥ thay ƒë·ªïi c·∫•u h√¨nh
    
    Returns:
        bool - True n·∫øu x√≥a cache th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    cache_key = "field_configs:required_optional"
    success = cache_delete(cache_key)
    return success


async def clear_llm_keys_cache(llm_id: int = None, key_type: str = None) -> bool:
    """
    X√≥a cache danh s√°ch API keys khi c√≥ thay ƒë·ªïi (th√™m, s·ª≠a, x√≥a key)
    
    Args:
        llm_id: ID c·ªßa LLM model. N·∫øu None, x√≥a cache cho t·∫•t c·∫£ LLMs
        key_type: Lo·∫°i key ("bot" ho·∫∑c "embedding"). N·∫øu None, x√≥a cache cho t·∫•t c·∫£ types
    
    Returns:
        bool - True n·∫øu x√≥a cache th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    from config.redis_cache import async_cache_delete
    
    try:
        if llm_id is not None:
            if key_type is not None:
                # X√≥a cache cho m·ªôt LLM v√† type c·ª• th·ªÉ
                cache_key = f"llm_keys:llm_id_{llm_id}:type_{key_type}"
                success = await async_cache_delete(cache_key)
                print(f"üóëÔ∏è ƒê√£ x√≥a cache {key_type} keys cho LLM id={llm_id}")
            else:
                # X√≥a cache cho t·∫•t c·∫£ types c·ªßa m·ªôt LLM
                for ktype in ["bot", "embedding", "all"]:
                    cache_key = f"llm_keys:llm_id_{llm_id}:type_{ktype}"
                    await async_cache_delete(cache_key)
                print(f"üóëÔ∏è ƒê√£ x√≥a cache t·∫•t c·∫£ keys cho LLM id={llm_id}")
                success = True
            return success
        else:
            # X√≥a cache cho t·∫•t c·∫£ (LLM id=1 l√† model ch√≠nh)
            for ktype in ["bot", "embedding", "all"]:
                cache_key = f"llm_keys:llm_id_1:type_{ktype}"
                await async_cache_delete(cache_key)
            print(f"üóëÔ∏è ƒê√£ x√≥a cache keys cho t·∫•t c·∫£ LLMs")
            return True
    except Exception as e:
        print(f"‚ùå L·ªói khi x√≥a cache keys: {e}")
        return False


async def clear_llm_model_cache() -> bool:
    """
    X√≥a cache th√¥ng tin model khi c√≥ thay ƒë·ªïi (c·∫≠p nh·∫≠t name, key m·∫∑c ƒë·ªãnh, etc.)
    
    Returns:
        bool - True n·∫øu x√≥a cache th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    from config.redis_cache import async_cache_delete
    
    try:
        cache_key = "llm_model_info:id_1"
        success = await async_cache_delete(cache_key)
        return success
    except Exception as e:
        print(f"‚ùå L·ªói khi x√≥a cache model: {e}")
        return False


async def generate_response_prompt(
    model,
    db_session: AsyncSession,
    query: str,
    chat_session_id: int,
    model_name: str = None
) -> str:
    """
    T·∫°o prompt cho bot response
    
    Args:
        model: LLM model ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
        db_session: AsyncSession - Database session
        query: str - C√¢u h·ªèi c·ªßa user
        chat_session_id: int - ID c·ªßa chat session
        model_name: str - T√™n model (gemini/gpt)
    
    Returns:
        str - Prompt ƒë√£ ƒë∆∞·ª£c t·∫°o
    """
    try:
        # L·∫•y l·ªãch s·ª≠ v√† th√¥ng tin kh√°ch h√†ng
        history = await get_latest_messages(db_session, chat_session_id, limit=10)
        
        # customer_info = await get_customer_infor(db_session, chat_session_id)
        
        if not query or query.strip() == "":
            return "N·ªôi dung c√¢u h·ªèi tr·ªëng, vui l√≤ng nh·∫≠p l·∫°i."
        
        # L·∫•y embedding key ri√™ng cho vi·ªác t·∫°o embedding
        model_info = await get_llm_model_info_cached(db_session)
        try:
            # L·∫•y embedding key v·ªõi Round-Robin
            embedding_key, embedding_key_name = await get_round_robin_api_key(
                model_info["id"],
                chat_session_id,
                db_session,
                key_type="embedding"
            )
            print(f"üîë S·ª≠ d·ª•ng embedding key: {embedding_key_name}")
        except ValueError as e:
            # Fallback v·ªÅ key m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ embedding key
            print(f"‚ö†Ô∏è Kh√¥ng c√≥ embedding key ri√™ng, d√πng key m·∫∑c ƒë·ªãnh: {e}")
            embedding_key = model_info["key"]
        
        # T√¨m ki·∫øm t√†i li·ªáu li√™n quan (s·ª≠ d·ª•ng embedding key)
        knowledge = await search_similar_documents(
            db_session, 
            query, 
            top_k=10,
            api_key=embedding_key,
            model_name=model_name
        )
        
        
        # L·∫•y c·∫•u h√¨nh fields
        # required_fields, optional_fields = await get_field_configs(db_session)
        
        # T·∫°o danh s√°ch th√¥ng tin c·∫ßn thu th·∫≠p
        # required_info_list = "\n".join([f"- {field_name} (b·∫Øt bu·ªôc)" for field_name in required_fields.values()])
        # optional_info_list = "\n".join([f"- {field_name} (t√πy ch·ªçn)" for field_name in optional_fields.values()])
        
        # Import prompt_builder t·ª´ prompt.py
        from llm.prompt import prompt_builder
        
        # T·∫°o prompt
        prompt = await prompt_builder(
            knowledge=knowledge,
            history=history,
            query=query
        )
        
        return prompt
        
    except Exception as e:
        print(f"‚ùå Error generating response: {e}")
        return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n: {str(e)}"




